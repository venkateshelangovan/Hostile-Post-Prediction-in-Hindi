{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Final_Result.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOmi0UJSbLn5"
      },
      "source": [
        "# Predicted Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he2WFOklZZRd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnnr9IQrZgdU"
      },
      "source": [
        "# Validation Data\n",
        "'''\n",
        "Loading Validation Data for which Coarse Grained Model \n",
        "(NonHostile vs Hostile Classification) predicted Hostile Class\n",
        "'''\n",
        "file = '/content/drive/My Drive/CONSTRAINT 2021 Projects (AAAI)/Hindi_Task/Dataset/hostile_validate_pred.xlsx'\n",
        "test_df = pd.read_excel(file)\n",
        "\n",
        "# Data Preparation into Pandas Dataframe for Model Input\n",
        "\n",
        "def get_data(a):\n",
        "  Unique_ID = list(a['Unique ID'])\n",
        "  sentence = list(a['Post'])\n",
        "  text_labels = list(a['Labels Set'])\n",
        "\n",
        "  label = []\n",
        "  for i in text_labels:\n",
        "    if i=='hostile':\n",
        "        label.append(0)\n",
        "\n",
        "  raw_data_train = {'UID':Unique_ID,'sentence': sentence, 'label': label}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['UID','sentence','label'])\n",
        "  return df\n",
        "\n",
        "test_data  = get_data(test_df)\n",
        "\n",
        "print(test_data[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um2aYOzrbabh"
      },
      "source": [
        "# Hostile ID Collection "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHN-feglaTQS"
      },
      "source": [
        "# Collecting Hostile IDs from Validation Data\n",
        "\n",
        "data = test_data\n",
        "\n",
        "hos_ids = []\n",
        "for i in range(len(data)):\n",
        "  id = data['UID'][i]\n",
        "  hos_ids.append(id)\n",
        "\n",
        "hos_ids = np.array(hos_ids, dtype=np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1XcSlsPbixS"
      },
      "source": [
        "# Predicted Labels Loading (Fake, Hate, Offensive, Defamation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGYARfJRbKCB"
      },
      "source": [
        "d_lab = np.load('/content/Final_Defamation_validation_Pred_Label.npy', allow_pickle=True)\n",
        "f_lab = np.load('/content/Final_Fake_validation_Pred_Label.npy', allow_pickle=True)\n",
        "h_lab = np.load('/content/Final_Hate_validation_Pred_Label.npy', allow_pickle=True)\n",
        "o_lab = np.load('/content/Final_Offensive_validation_Pred_Label.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0eSzzsZcAPI"
      },
      "source": [
        "# Merging Labels to Asses Performance measurement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5CH0c26brAh"
      },
      "source": [
        "# Merging Predicted labels into a single numpy array\n",
        "# Reference: [non_hostile,defamation,fake,hate,offensive]\n",
        "\n",
        "predicted_labels = []\n",
        "\n",
        "count = 0\n",
        "for i in range(1,812):\n",
        "  row = []\n",
        "  if i not in hos_ids:\n",
        "    row.append([1,0,0,0,0])\n",
        "  else:\n",
        "    alt_row = [0,0,0,0,0]\n",
        "    if d_lab[count]==1:\n",
        "      alt_row[1] = 1\n",
        "    if f_lab[count]==1:\n",
        "      alt_row[2] = 1\n",
        "    if h_lab[count]==1:\n",
        "      alt_row[3] = 1\n",
        "    if o_lab[count]==1:\n",
        "      alt_row[4] = 1\n",
        "    count += 1\n",
        "    row.append(alt_row)\n",
        "  predicted_labels.append(row)\n",
        "\n",
        "pred_lab = np.reshape(np.array(predicted_labels),(811,5)) # Final Predicted Labels\n",
        "np.save('Final_Pred_Labels_Validation.npy',pred_lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoxaPTAdcSPw"
      },
      "source": [
        "# Loading True Labels (811,5) and Predicted Labels (811,5) for Final Result Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfok1L51b_W4"
      },
      "source": [
        "y_true = np.load('/content/True_Validation_labels.npy', allow_pickle=True)       # Provided by Organizers\n",
        "y_pred = np.load('/content/Final_Pred_Labels_Validation.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjQmo3w6cdqh"
      },
      "source": [
        "# Computing Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSRn5MG5cbt6"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(y_true, y_pred, average='macro'))\n",
        "print(f1_score(y_true, y_pred, average='micro'))\n",
        "print(f1_score(y_true, y_pred, average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xksjw8kCclP4"
      },
      "source": [
        "# Creating Final Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLC1RLkacgLh"
      },
      "source": [
        "# Reference: [non_hostile,defamation,fake,hate,offensive]\n",
        "labels = []\n",
        "\n",
        "for i in range(1,y_pred.shape[0]+1):       \n",
        "  idx = np.argwhere(y_pred[i]>0)\n",
        "  idx = idx.reshape(idx.shape[0],)\n",
        "  if (len(idx)==0):\n",
        "    lab_text.append('defamation')\n",
        "  else:\n",
        "  for j in idx:\n",
        "    if j==0:\n",
        "      lab_text.append('non-hostile')\n",
        "    if j==1:\n",
        "      lab_text.append('defamation')\n",
        "    if j==2:\n",
        "      lab_text.append('fake')\n",
        "    if j==3:\n",
        "      lab_text.append('hate')\n",
        "    if j==4:\n",
        "      lab_text.append('offensive')\n",
        "      \n",
        "labels.append(lab_text)\n",
        "\n",
        "def final_submission(label_list):\n",
        "  import csv\n",
        "  data = []\n",
        "  titles = ['Unique ID','Labels Set']\n",
        "  data.append(titles)\n",
        "  for i in range(len(label_list)):\n",
        "    row = []\n",
        "    row.append(i+1)\n",
        "    lab_text = ''\n",
        "    for j in range(len(label_list[i])):\n",
        "      lab_text += str(label_list[i][j])+','\n",
        "    lab_text = lab_text[:-1]+''\n",
        "    row.append(str(lab_text))\n",
        "    data.append(row)\n",
        "\n",
        "  file1 = \"answer.csv\"\n",
        "  with open(file1, 'w') as csvfile:  \n",
        "    csvwriter = csv.writer(csvfile)   \n",
        "    csvwriter.writerows(data)\n",
        "\n",
        "final_submission(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JckjowZrGgYE"
      },
      "source": [
        "# Score with Official Evaluation Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HweUhgSmGe9d"
      },
      "source": [
        "### Order of Labels --> [Hostile,defamation,fake,hate,offensive,non-hostile]\r\n",
        "### An example      --> [1,0,1,1,0,0]\r\n",
        "\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def preprocess(df):\r\n",
        "    \r\n",
        "    df = df.dropna()\r\n",
        "    \r\n",
        "    df.insert(len(df.columns)-1,'Hostile', np.zeros(len(df),dtype=int))\r\n",
        "    df.insert(len(df.columns)-1,'Defamation', np.zeros(len(df),dtype=int))\r\n",
        "    df.insert(len(df.columns)-1,'Fake', np.zeros(len(df),dtype=int))\r\n",
        "    df.insert(len(df.columns)-1,'Hate', np.zeros(len(df),dtype=int))\r\n",
        "    df.insert(len(df.columns)-1,'Offensive', np.zeros(len(df),dtype=int))\r\n",
        "    df.insert(len(df.columns)-1,'Non-Hostile', np.zeros(len(df),dtype=int))    \r\n",
        "    \r\n",
        "    for i in range(len(df)):\r\n",
        "        text = df['Labels Set'][i]\r\n",
        "        text = text.lower()\r\n",
        "        text = text.replace('\\n',\"\")\r\n",
        "        text = text.replace('\"',\"\")\r\n",
        "        text = text.replace(\" \",\"\")\r\n",
        "        text = text.split(',')\r\n",
        "\r\n",
        "\r\n",
        "        for word in text:\r\n",
        "            if word == 'defamation':\r\n",
        "                df.at[i,'Hostile']    = 1\r\n",
        "                df.at[i,'Defamation'] = 1\r\n",
        "    \r\n",
        "            if word == 'fake':\r\n",
        "                df.at[i,'Hostile']    = 1\r\n",
        "                df.at[i,'Fake'] = 1\r\n",
        "    \r\n",
        "            if word == 'hate':\r\n",
        "                df.at[i,'Hostile']    = 1\r\n",
        "                df.at[i,'Hate'] = 1\r\n",
        "    \r\n",
        "            if word == 'offensive':\r\n",
        "                df.at[i,'Hostile']    = 1\r\n",
        "                df.at[i,'Offensive'] = 1\r\n",
        "    \r\n",
        "            if word == 'non-hostile' and df['Hostile'][i]==0:\r\n",
        "                df.at[i,'Hostile']    = 0\r\n",
        "                df.at[i,'Non-Hostile'] = 1\r\n",
        "\r\n",
        "    return df \r\n",
        "  \r\n",
        "    \r\n",
        "def get_scores(y_true, y_pred):\r\n",
        "    \r\n",
        "    hostility_true = y_true['Hostile']\r\n",
        "    hostility_pred = y_pred['Hostile']\r\n",
        "    \r\n",
        "    hostility_f1 = f1_score(y_true=hostility_true, y_pred=hostility_pred, average='weighted')\r\n",
        "    \r\n",
        "    \r\n",
        "    nh_indexes = y_true[y_true['Hostile']==0].index\r\n",
        "    y_true = y_true.drop(nh_indexes)\r\n",
        "    y_true = y_true.reset_index(drop=True)\r\n",
        "    \r\n",
        "    y_pred = y_pred.drop(nh_indexes)\r\n",
        "    y_pred = y_pred.reset_index(drop=True)\r\n",
        "    \r\n",
        "    \r\n",
        "    fine_true = y_true[['Defamation','Fake','Hate','Offensive']]\r\n",
        "    fine_pred = y_pred[['Defamation','Fake','Hate','Offensive']]\r\n",
        "    \r\n",
        "    \r\n",
        "    fine_f1          = f1_score(y_true=fine_true, y_pred=fine_pred, average=None)\r\n",
        "    defame_f1        = fine_f1[0]\r\n",
        "    fake_f1          = fine_f1[1]\r\n",
        "    hate_f1          = fine_f1[2]\r\n",
        "    offensive_f1     = fine_f1[3]\r\n",
        "    weighted_fine_f1 = f1_score(y_true=fine_true, y_pred=fine_pred, average='weighted')\r\n",
        "\r\n",
        "    return [hostility_f1, defame_f1, fake_f1, hate_f1, offensive_f1, weighted_fine_f1]\r\n",
        "\r\n",
        "ground_truth_path      = 'content\\Validation Ground Truth.csv'\r\n",
        "submission_file_path   = 'content\\answer.csv\"\r\n",
        "\r\n",
        "try:  \r\n",
        "    y_true = pd.read_csv(ground_truth_path)\r\n",
        "    y_pred = pd.read_csv(submission_file_path)\r\n",
        "    \r\n",
        "    y_true = preprocess(y_true)\r\n",
        "    y_pred = preprocess(y_pred)\r\n",
        "    \r\n",
        "    team_score = get_scores(y_true,y_pred)\r\n",
        "    \r\n",
        "    \r\n",
        "except:\r\n",
        "    team_score = [0,0,0,0,0,0]\r\n",
        "    \r\n",
        "        \r\n",
        "print(\"Coarse Grained F1-score: \", team_score[0])\r\n",
        "print(\"Defamation F1-score:     \", team_score[1])\r\n",
        "print(\"Fake F1-score:           \", team_score[2])\r\n",
        "print(\"Hate F1-score:           \", team_score[3])\r\n",
        "print(\"Offensive F1-score:      \", team_score[4])\r\n",
        "print(\"Fine Grained F1-score:   \", team_score[5])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}